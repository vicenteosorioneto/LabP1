# Self-Attention - ImplementaÃ§Ã£o Educacional

Uma implementaÃ§Ã£o clara e bem-documentada do mecanismo de Self-Attention, elemento fundamental dos Transformers modernos.

## ğŸ“‹ CaracterÃ­sticas

âœ… **CÃ³digo limpo e legÃ­vel** - Nomes de variÃ¡veis descritivos (ex: `query_matrix`, `attention_weights`)  
âœ… **Tipagem completa** - Type hints em todo o cÃ³digo  
âœ… **Docstrings robustas** - DocumentaÃ§Ã£o detalhada de classes e mÃ©todos  
âœ… **ValidaÃ§Ã£o de entrada** - Tratamento abrangente de erros de dimensÃ£o  
âœ… **Benchmark incluÃ­do** - ComparaÃ§Ã£o de performance para diferentes tamanhos  
âœ… **Suite de testes extensiva** - 16+ testes validando comportamento  

## ğŸš€ InÃ­cio RÃ¡pido

### InstalaÃ§Ã£o

```bash
# SÃ³ precisa do NumPy
pip install numpy
```

### Uso BÃ¡sico

```python
import numpy as np
from attention import SelfAttention

# Cria o mÃ³dulo de atenÃ§Ã£o
attention = SelfAttention()

# Prepara dados (seq_length=2, feature_dim=3)
query_matrix = np.array([[1.0, 0.0, 1.0],
                         [0.0, 1.0, 0.0]], dtype=np.float32)
key_matrix = np.array([[1.0, 0.0, 1.0],
                       [0.0, 1.0, 0.0]], dtype=np.float32)
value_matrix = np.array([[1.0, 2.0],
                        [3.0, 4.0]], dtype=np.float32)

# Calcula atenÃ§Ã£o
attention_output, attention_weights = attention.forward(
    query_matrix, key_matrix, value_matrix
)

print("Output shape:", attention_output.shape)  # (2, 2)
print("Pesos:\n", attention_weights)
print("Output:\n", attention_output)
```

## ğŸ“ Arquitetura

### Self-Attention

A fÃ³rmula matemÃ¡tica implementada:

```
Attention(Q, K, V) = softmax(QÂ·K^T / âˆšd_k) Â· V
```

Onde:
- **Q** (Query): O que estamos procurando
- **K** (Key): O que temos disponÃ­vel
- **V** (Value): A informaÃ§Ã£o que queremos recuperar  
- **d_k**: DimensÃ£o das chaves (usado para escaling)

### Fluxo de CÃ¡lculo

```
1. Scores = Q @ K^T              # Compatibilidade entre queries e keys
2. Scaled = Scores / âˆšd_k        # Normalization
3. Weights = softmax(Scaled)     # Probabilidades de atenÃ§Ã£o
4. Output = Weights @ V          # AgregaÃ§Ã£o ponderada dos valores
```

## ğŸ“Š Exemplos de Uso

### Com MÃ¡scara Causal (Para GeraÃ§Ã£o Sequencial)

```python
import numpy as np
from attention import SelfAttention

attention = SelfAttention()
seq_length = 5

# Dados de sequÃªncia
query = np.random.randn(seq_length, 64)
key = np.random.randn(seq_length, 64)
value = np.random.randn(seq_length, 32)

# MÃ¡scara causal: posiÃ§Ã£o i nÃ£o pode atender posiÃ§Ãµes j > i
causal_mask = np.tril(np.ones((seq_length, seq_length), dtype=bool))

output, weights = attention.forward(query, key, value, mask=causal_mask)

print("Pesos com mÃ¡scara:")
print(weights)
# Valores acima da diagonal serÃ£o 0
```

### Com Diferentes DimensÃµes

```python
# SequÃªncia de 100 tokens, embedding 512, output 256
query = np.random.randn(100, 512)
key = np.random.randn(100, 512)
value = np.random.randn(100, 256)  # Output dimension pode diferir

output, weights = attention.forward(query, key, value)

assert output.shape == (100, 256), "Output tem dimensÃ£o correta"
assert weights.shape == (100, 100), "Weights Ã© uma matriz quadrada"
```

## âœ‹ Tratamento de Erros

A implementaÃ§Ã£o valida entradas automaticamente:

```python
# âŒ Dimension mismatch (Q e K tÃªm dimensÃµes diferentes)
query = np.random.randn(5, 8)
key = np.random.randn(5, 16)  # Erro!
value = np.random.randn(5, 8)
attention.forward(query, key, value)  # Levanta ValueError

# âŒ Input nÃ£o Ã© numpy array
query = [[1.0, 2.0], [3.0, 4.0]]  # Lista, nÃ£o array
attention.forward(query, key, value)  # Levanta TypeError

# âŒ SequÃªncia vazia
query = np.random.randn(0, 8)
attention.forward(query, key, value)  # Levanta ValueError
```

## ğŸ§ª Testes

Executa a suite completa de testes:

```bash
python test_attention.py
```

Inclui testes para:

- âœ“ Formas de saÃ­da corretas
- âœ“ Propriedades matemÃ¡ticas (softmax soma a 1)
- âœ“ ValidaÃ§Ã£o de dimensÃµes
- âœ“ MÃ¡scara causal
- âœ“ Diferentes tamanhos de sequÃªncia
- âœ“ Tratamento de erros
- âœ“ Benchmark de performance

### SaÃ­da Esperada

```
======================================================================
TESTES DE SELF-ATTENTION
======================================================================

[TESTES BÃSICOS]
âœ“ Forward pass retorna shape correto
âœ“ Pesos de atenÃ§Ã£o tÃªm shape correto
...

[BENCHMARK DE PERFORMANCE]
Tamanho      | Seq Len | Features | Tempo (ms) | Ops/sec
------------------------------------------------------------
Pequeno      |      10 |       64 |      0.234 |   27.45G
MÃ©dio        |     100 |      256 |      2.156 |   24.32G
...

======================================================================
RESULTADO: 16/16 testes passaram (100.0%)
======================================================================
```

## ğŸ“ˆ Performance

Benchmark de velocidade em diferentes configuraÃ§Ãµes:

| Tamanho | Seq Length | Features | Tempo (ms) |
|---------|-----------|----------|-----------|
| Pequeno | 10 | 64 | ~0.2 |
| MÃ©dio | 100 | 256 | ~2.0 |
| Grande | 500 | 512 | ~50.0 |
| Muito Grande | 1000 | 1024 | ~400.0 |

*Valores indicativos em CPU. GPU seria muito mais rÃ¡pida.*

## ğŸ“š Conceitos-Chave

### Por que Escaling?

O scaling por âˆšd_k previne que os gradientes desapareÃ§am durante backpropagation:

```python
# Sem scaling: scores muito grandes
scores = Q @ K.T  # Valores em range [-1000, 1000]
weights = softmax(scores)  # Picos muito agudos, gradientes â†’ 0

# Com scaling: scores moderados  
scores = (Q @ K.T) / sqrt(d_k)  # Valores em range [-5, 5]
weights = softmax(scores)  # DistribuiÃ§Ã£o suave, gradientes bons
```

### Estabilidade NumÃ©rica

O softmax Ã© implementado com subtraÃ§Ã£o do mÃ¡ximo para evitar overflow:

```python
# âŒ Problematisch (pode overflow)
exp_scores = np.exp(scores)

# âœ… EstÃ¡vel (numericamente seguro)
shifted = scores - np.max(scores, axis=1, keepdims=True)
exp_scores = np.exp(shifted)
```

### MÃ¡scara Causal

Essencial para modelos autoregressivos (como GPT):

```
Sem mÃ¡scara:        Com mÃ¡scara:
[1, 1, 1]          [1, 0, 0]
[1, 1, 1]          [1, 1, 0]
[1, 1, 1]          [1, 1, 1]

Cada posiÃ§Ã£o vÃª tudo    Cada posiÃ§Ã£o vÃª apenas anteriores
```

## ğŸ”§ Estrutura do CÃ³digo

```
self-attention/
â”œâ”€â”€ attention.py        # ImplementaÃ§Ã£o da classe SelfAttention (160+ linhas)
â”œâ”€â”€ test_attention.py   # Suite de testes (400+ linhas)
â””â”€â”€ README.md          # Este arquivo
```

### Arquivos

**attention.py:**
- `SelfAttention`: Classe principal
  - `forward()`: Calcula atenÃ§Ã£o
  - `_validate_input_dimensions()`: Valida entrada
  - `_softmax()`: Softmax estÃ¡vel

**test_attention.py:**
- `TestSelfAttention`: Suite de testes
  - 16+ testes unitÃ¡rios
  - Benchmark de performance
  - ValidaÃ§Ã£o de erros

## ğŸ’¡ PrÃ³ximos Passos

Para expandir este projeto:

1. **Multi-Head Attention** - MÃºltiplas cabeÃ§as em paralelo
2. **Scaled & Position Encoding** - Adicionar posiÃ§Ãµes na sequÃªncia
3. **PyTorch Version** - ImplementaÃ§Ã£o com autograd
4. **Causal + Padding Mask** - MÃ¡scaras combinadas
5. **Cross-Attention** - AtenÃ§Ã£o entre sequÃªncias diferentes

## ğŸ“– ReferÃªncias

- "Attention is All You Need" (Vaswani et al., 2017)
- https://arxiv.org/abs/1706.03762

## ğŸ“ LicenÃ§a

CÃ³digo educacional - Livre para usar e modificar.

---

**Autor:** LaboratÃ³rio de ProgramaÃ§Ã£o (LabP1)  
**Data:** 2026  
**Status:** âœ… Pronto para uso
